@article{Sloman1984TheMinds,
    title = {{The Structure of the Space of Possible Minds}},
    year = {1984},
    author = {Sloman, Aaron},
    pages = {35--42}
}

@article{corrigibility,
author = {Soares, Nate and Fallenstein, Benja and Yudkowsky, Eliezer and Armstrong, Stuart},
journal = {AAAI Workshop on AI and Ethics},
number = {2014},
pages = {74--82},
title = {{Corrigibility}},
year = {2015}
}

@article{concrete_problems,
  author    = {Dario Amodei and
               Chris Olah and
               Jacob Steinhardt and
               Paul Christiano and
               John Schulman and
               Dan Man{\'{e}}},
  title     = {Concrete Problems in {AI} Safety},
  journal   = {CoRR},
  volume    = {abs/1606.06565},
  year      = {2016},
}

@article{bostrom2003ethical,
  title={Ethical issues in advanced artificial intelligence},
  author={Bostrom, Nick},
  journal={Science Fiction and Philosophy: From Time Travel to Superintelligence},
  pages={277--284},
  year={2003}
}

@article{basic_ai_drives,
abstract = {One might imagine that AI systems with harmless goals will be harmless. This paper instead shows that intelligent systems will need to be carefully designed to prevent them from behaving in harmful ways. We identify a number of " drives " that will appear in sufficiently advanced AI systems of any design. We call them drives because they are tendencies which will be present unless explicitly coun-teracted. We start by showing that goal-seeking systems will have drives to model their own operation and to improve themselves. We then show that self-improving systems will be driven to clarify their goals and represent them as economic utility functions. They will also strive for their actions to approximate rational economic behavior. This will lead almost all systems to protect their utility functions from modification and their utility measurement systems from corruption. We also dis-cuss some exceptional systems which will want to modify their utility functions. We next discuss the drive toward self-protection which causes systems try to pre-vent themselves from being harmed. Finally we examine drives toward the acqui-sition of resources and toward their efficient utilization. We end with a discussion of how to incorporate these insights in designing intelligent technology which will lead to a positive future for humanity.},
author = {Omohundro, Stephen M},
keywords = {Artificial Intelligence,Cognitive Drives,Rational Economic Behavior,Self-Improving Systems,Utility Engineering},
title = {{The Basic AI Drives}}
}

@article{Marsh1994FormalisingConcept,
    title = {{Formalising Trust as a Computational Concept}},
    year = {1994},
    journal = {Computing},
    author = {Marsh, Stephen Paul},
    number = {April},
    pages = {184},
    volume = {Doctor of},
    isbn = {CSM-133},
    doi = {10.2165/00128413-199409230-00010}
}

@inproceedings{eigentrust,
  title={The eigentrust algorithm for reputation management in p2p networks},
  author={Kamvar, Sepandar D and Schlosser, Mario T and Garcia-Molina, Hector},
  booktitle={Proceedings of the 12th international conference on World Wide Web},
  pages={640--651},
  year={2003},
  organization={ACM}
}

@article{CastelfranchiSocialApproach,
    title = {{Social Trust: A Cognitive Approach}},
    author = {Castelfranchi, Cristiano and Falcone, Rino},
    year = {2001}
}

@article{luhmann2000familiarity,
  title={Familiarity, confidence, trust: Problems and alternatives},
  author={Luhmann, Niklas},
  year={2000}
}

@article{deutsch1962cooperation,
  title={Cooperation and trust: Some theoretical notes.},
  author={Deutsch, Morton},
  year={1962},
  publisher={Univer. Nebraska Press}
}

@article{Marsh2011,
abstract = {Device Comfort is a concept that uses an enhanced notion of trust to allow a personal (likely mobile) device to better reason about the state of interactions and actions between it, its owner, and the environment. This includes allowing a better understanding of how to manage information in fine-grained context as well as addressing the personal security of the user. To do this, it forms a unique relationship with the user, focusing on the device's judgment of user in context. This paper introduces and defines Device Comfort, including an examination of what makes up the comfort of a device in terms of trust and other considerations, and discusses the uses of such an approach. It also presents some ongoing developmental work in the concept, and an initial formal model of Device Comfort, its makeup and behaviour.},
author = {Marsh, Stephen and Briggs, Pamela and El-Khatib, Khalil and Esfandiari, Babak and Stewart, John A.},
doi = {10.2197/ipsjjip.19.231},
file = {:Users/tom/Documents/Papers/IPSJ-JNL5207004.pdf:pdf},
issn = {1882-6652},
journal = {Journal of Information Processing},
number = {7},
pages = {231--252},
title = {{Defining and Investigating Device Comfort}},
url = {http://ci.nii.ac.jp/naid/110008508036/en/},
volume = {19},
year = {2011}
}

@misc{armstrong_bias,
  author = {Armstrong, Stuart},
  title = {Fairness in Machine Learning Decisions},
  howpublished = {\href{http://bit.ly/2hCYjdB}{http://bit.ly/2hCYjdB}},
  year = {2016},
  note = "[online; accessed 30-december-2016]"
}

@unpublished{wallis_responsibility,
  author = {Wallis, William},
  title = {On Computational Responsibility},
  note = {Masters thesis currently in progress},
  year = {2016}
}

@book{Gambetta1988,
	year = {1988},
	author = {Diego Gambetta},
	title = {Trust: Making and Breaking Cooperative Relations},
	publisher = {Blackwell}
}

@article{fitts1954information,
  title={The information capacity of the human motor system in controlling the amplitude of movement.},
  author={Fitts, Paul M},
  journal={Journal of experimental psychology},
  volume={47},
  number={6},
  pages={381},
  year={1954},
  publisher={American Psychological Association}
}

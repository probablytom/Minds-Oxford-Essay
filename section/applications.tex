\section{Applications of the Theory}
The theory proposed can be applied in several ways, due to its interdisciplinary nature. Detailed below are some examples, focusing on the problem of AI Safety as an example of its utility in solving practical, concrete problems in a largely theoretical field.\par

\subsection{Attacking Concrete AI Safety Problems}

\subsubsection{Reward Hacking}
A list of concrete AI safety problems researchers could work on solving at present is described in~\cite{concrete_problems}. Among those is the problem of Reward Hacking. In~\cite{concrete_problems}, reward hacking is introduced as:

\begin{displayquote}
In ``reward hacking'', the objective function that the designer [of the AI] writes down admits of some clever ``easy'' solution that formally maximizes it but perverts the spirit of the designerâ€™s intent (i~.e~. the objective function can be ``gamed''), a generalization of the wireheading problem.
\end{displayquote}

Reward hacking exists in humans. For example, athletes will on occasion make use of performance enhancing drugs --- their reward function is fulfilled by winning gold medals, but the goal itself isn't technically fulfilled if it makes use of a loophole in some regulation of a sport.\par

There are behavioural tendencies which limit the inclination of a mind to reward hack. Examples of this might include social responsibility or a decreased degree of comfort in actions which harm other agents in some way. Happily, computational comfort formalisms are already being developed~\citep{Marsh2011}, and computational responsibility formalisms are in their early stages~\citep{wallis_responsibility}.\par

\subsubsection{Corrigibility}
\todo[inline]{Pay attention to this when editing! This section will catch eyes in FHI. It's important to get right, and you're writing it rather tired!}
The problem of an agent's corrigibility is defined in~\cite{corrigibility} as:

\begin{displayquote}
We call an AI system ``corrigible'' if it cooperates with what its creators regard as a corrective intervention, despite default incentives for rational agents to resist attempts to shut them down or modify their preferences.
\end{displayquote}

Again, analysing human behaviour regarding corrigibility allows one to see what concepts can be transplanted to an intelligent agent. One is inclined to identify traits of humans which we associate with corrigibility, so as to maximise this trait --- but a formalism of incorrigible traits might be equally useful and present an alternative angle of attack.\par

To demonstrate: incorrigible human agents might be described as ``stubborn''. Once they have a certain goal in mind, they are unlikely to shift it. The intelligent agent equivalent of this is that they appear naturally stubborn: sufficiently intelligent agents would identify that, when selecting an action which alters its utility function, the change does not help it to achieve its present goal. It is therefore unlikely to change its goal --- maximally stubborn. A formalism of stubbornness would describe this behaviour in a quantifiable way; a corrigible agent would then minimise values of its stubbornness formalism when assessing decisions.\par

One might note that this approach has a flaw: a human who minimises this definition of stubbornness would be easily persuaded to change their goals on a whim and for little reason. Regarding a superintelligent agent, this is evidently a serious safety risk for human actors\footnote{Limitations of the theory are discussed in \cref{sec:limitations}.}. A tempting inclination is to fix these issues with further anthropomorphic analysis: for example, a responsibility formalism might weight non-stubborn decisions against whether the change appears to discharge (or prevent the discharge) of responsibilities the intelligent agent has. An equillibrium between the behaviours of responsibility and stubbornness would therefore be ideal, and stable combinations of different anthropomorphic traits can be expected to feature in solving similar instances of this dilemma.\par

These approaches in turn have flaws and edge cases, but stray agents can be made more corrigible through these methods precisely because the theory provided allows one to specify the \emph{behaviour} of an agent. Agents can therefore be made corrigible in more, if not all, scenarios.\par

\subsection{Anthropology}
Refining and developing formalisms of behaviour of certain groups of people is a task often taken on by sociologists and psychologists~\citep{Gambetta1988, luhmann2000familiarity}. However, as these formalisms become more advanced, they permit two applications for modern anthropology.\par

First, anthropologists may use the formalisms to predict the behaviour of new cultures, as well as how those cultures might shift, as more socially involved artificial minds interact with human minds more frequently. Studying this is important, as the impact of minds with a subset of human traits might be significant: a greater emphasis might be put on social constructs of respect, for example, should a collection of people begin using respect as an interaction mechanism with the technology around them. \par

It is not unknown for technological interaction to put more --- or less --- emphasis on the aspects of human culture it touches. For example, modern communication techniques such as instant messaging alter the dynamic of communication: within the space of only a century or two, the expected time to wait for a response to a message has shortened from the days it took to respond to a letter to the minutes it takes to read and reply to an instant message. In turn, people's expectations about the promptness of communication alter accordingly; therefore technological innovation can have a marked and significant impact on culture. Introducing artificial minds with a subset of human traits lends a ripe field of study for anthropologists to research.\par

Secondly, when observing new types of culture and shifts in common cultures, anthropologists will begin to note how some fundamental human traits --- rather than emergent phenomena of the ordinary human mind --- shift over time. This will help to:

\begin{enumerate}
    \item Understand more of the nature of Sloman's space, as it pertains to the fundamental traits of a human mind
    \item Understand more of the structure of the human subspace of minds, and what different types of human minds might exist
\end{enumerate}

The utility of this work to the philosopher, and to AI Safety research, is that integrating empirical observations about the world allows for more informed and practical thought experiments or practical exercises in AI~. \par

The theory then applies to anthropological study in the deeper understanding of the human mind from a cultural perspective, and a pragmatic framework by which anthropologists might note precisely what sorts of minds their field studies. It might also permit anthropological study to broaden, from humans and their culture to minds with human-like traits --- observing how minds with human properties might engage with culture, and what traits affect culture in different ways. This practice has the added benefit of the anthropological study contributing to the original theory, strengthening empirical evidence which can be used in the study of AI and its safety, as well as lending practical applications to non-reductive physical theories of mind through their working in tandem with the theory presented.\par

\subsection{Human-Computer Interaction} % The machine isn't the only thing being formalised. Interaction from both angles can be described more formally, which is important given there's not much more than Fitt's law to HCI formalisms & mathematics. 
A better understanding of the formalism of minds does more than bolster our understanding of artificial minds: it also helps to perceive how a human operates and therefore, how interfaces between artificial minds and human minds might be better achieved.\par

Currently, much human-computer interaction research centres around interaction mechanisms and design for input and output devices: styluses, spherical displays, phone applications and so on. Application of this formalism-centric approach, however, would permit a human-computer interaction paradigm which centres around how ``feelings'' change: humans and computers might work together to build trust, or to avoid danger through a sense of fear, as these are all human behaviours which can theoretically be formalised and applied to an artificial mind.\par

An example might be trust regards privacy: a user's phone might present fewer security barriers to accessing sensitive information when being accessed in a hospital, for example, if the data accessed was related to health. Then, sensitive medical data can be unlocked by the trust in a user or location which can be learned over time; if a hospital shuts down and medical professionals are no longer accessing data from the location, trust in that location would dwindle just as human trust in old notions might dwindle over time also.\par

In the field of human-computer interaction, it's unusual for rigorous mathematical study to take place; empirical ``laws'' of interaction, such as Fitt's Law~\citep{fitts1954information}, are rare. However, this theory presents an approach which, with research into the space's structure, present another mathematical and rigorous study of interaction.

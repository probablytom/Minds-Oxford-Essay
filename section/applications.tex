\section{Applications of the Theory}
The theory proposed can be applied in several ways, due to its interdisciplinary nature. Detailed below are some examples, focusing on the problem of AI Safety as an example of its utility in solving practical, concrete problems in a largely theoretical field.\par

\subsection{Attacking Concrete AI Safety Problems}

\subsubsection{Reward Hacking}
A list of concrete AI safety problems researchers could work on solving at present is described in~\cite{concrete_problems}. Among those is the problem of Reward Hacking. In~\cite{concrete_problems}, reward hacking is introduced as:

\begin{displayquote}
In ``reward hacking'', the objective function that the designer [of the AI] writes down admits of some clever ``easy'' solution that formally maximizes it but perverts the spirit of the designer’s intent (i.e. the objective function can be ``gamed''), a generalization of the wireheading problem.
\end{displayquote}

Reward hacking exists in humans. For example, some people are known to pirate software: in this instance, the objective function is to acquire software, and this is achieved through piracy. It also allows the human to acquire \emph{more} software, as piracy uses far less resources than paying for computer programs --- this is akin to the formal maximisation quoted. This circumvents the societally intended method for acquiring software, however, which is to pay for it. As a result, the developer of the software is left unsupported for the work put into creating the pirated software.\par

Often, humans will not pirate software and will instead pay. In some scenarios, this behaviour is made more likely through adapting the context of the goal: operating systems with easier-to-use stores for software, or tight measures for signature and checking of authenticity of a program to discourage piracy. Other humans are less inclined to pirate than other, however --- there are therefore behavioural tendencies which limit the inclination of a mind to reward hack. Examples of this might include social responsibility or a decreased degree of comfort in actions which harm other agents in some way. Happily, computational comfort formalisms are already being developed~\citep{Marsh2011}, and computational responsibility formalisms are currently being worked on~\cite{wallis_responsibility}.\par

\subsubsection{Corrigibility}
The problem of an agent's corrigibility is defined in~\cite{corrigibility} as:

\begin{displayquote}
We call an AI system “corrigible” if it cooperates with what its creators regard as a corrective intervention, despite default incentives for rational agents to resist attempts to shut them down or modify their preferences.
\end{displayquote}



\subsection{Anthropology}


\subsection{Human-Computer Interaction} % The machine isn't the only thing being formalised. Interaction from both angles can be described more formally, which is important given there's not much more than Fitt's law to HCI formalisms & mathematics. 

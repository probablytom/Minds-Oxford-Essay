\subsection{An Example: The Electric Monk}
In the novel ``Dirk Gently's Holistic Detective Agency'' by Douglas Adams, there is presented a character who is useful as an example of anthropomorphic algorithms at work. ``The Electric Monk'' is a commercial artificial intelligence which is designed for the purpose of alleviating the burden of ``believing'' in things from the owner, similarly to how a television recorder might ``watch television'' for the owner.\par

This Electric Monk is responsible for killing a local businessman, due to its being told to ``shoot off'' (as in go away) and taking the instruction literally. It then believed that it was imperative that it shoot something.\par

The Electric Monk is an excellent example of anthropomorphic algorithms' strengths and weaknesses. Its limited anthropomorphism presents some utility, but its intelligence also makes it dangerous. As a result of its limited anthropomorphism, it lacks the capacity to act in guaranteed safe ways. One is inclined to explore how anthropomorphic algorithms might be applied safely in this case.\par

\subsubsection{Incitement to act}
The Electric Monk would be harmless if it was incapable of \emph{action}. In reality, the monk's prescribed activity --- that of belief --- is an action with no output, similar to a speech act\footnote{A speech act is the sort of action discharged implicitly when certain things are spoken, but which have no tangible effect. An example would be ``I now pronounce you man and wife'' --- this is rather meaningless if spoken by most people, but by a priest on an altar in front of a man and a woman, the words cause something to happen which is non-tangible. The Electric Monk's belief is similar: it has no impact physically, and no output, but it has value in the fictional world.}. An agent which need produce no output, then, might be limited such that it cannot act.\par

Of course, this limitation is meaningless for almost all realistic agents; certainly, intelligent agents can be made safe by neutering their ability to do anything at all, but one supposes this would strip the utility from them somewhat. How can we make agents which are incited to act safe?\par

One way to do this would be to observe what traits humans rely on for the safety of their actions, and to implement formalisms of those traits. So, if the Electric Monk were allowed to speak, traits humans use for safety in this case would be implemented. These traits might include responsibility for their actions, and an ethical understanding taking into account expected consequence and the net positive or negative effect of those actions. By characterising the traits humans use when acting, and implementing those traits, one can imagine that the artificial agent would be at least as safe as an equivalent human agent\footnote{And potentially safer, as it would likely be be possible to implement the formalisms with more accuracy than a human agent might --- humans can ignore these traits, but no such ability might be made available to the intelligent agent}.\par

\subsubsection{More formalisms necessary}
The Electric Monk's belief formalism is all that at acts on, but this is also the only formalism it is given; it shows little other human-like behaviour (save walking and talking). It is apparent that a single formalism might often be abused so as to permit unsafe behaviour. An emotional agent might understand happiness and sadness in human-like ways, so as to maximise happiness: this is dangerous in the same way as the paperclip maximiser.\par

However, Electric Monk-like agents will surely come to be in the future, particularly with the development of general intelligence. Therefore, the combination of multiple formalisms in any anthropomorphic agent seems an appropriate safety precaution. This way, an equilibrium might be reached though nuances of the behaviours' interactions together. For example, a happiness-maximising agent is effectively a utilitarian agent\todo{Is this correct terminology?}, but real-world utilitarians might be limited by laziness --- preventing those human agents from performing tasks which are lots of effort. Effort formalisms might limit the safety issues concerning a utilitarian agent in this case. \par

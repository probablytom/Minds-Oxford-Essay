\section{Limitations and Solutions}\label{sec:limitations}
As it stands, the theory proposed has several useful applications. This is not to say, however, that it presents a complete and foolproof measure of how minds might interact in Sloman's space, nor is the theory as complete as one might hope it to be. These issues, regardless of their significance presently, do not seem to be ultimately fatal to the theory. We shall explore some of these issues now, and discuss future work which will address some issues with behaviour formalisms in the coming section (\cref{sec:future_work})

% THis theory of mind requires functional states to give rise to emergent phenomena: these emergent phenomena are what the behaviour formalisms are based on, so functionalism is essential for formalisms to apply appropriately to humans and to computers.
\subsection{Incompatibility with certain Theories of Mind}\label{subsec:incompatibility}
One criticism of the proposed theory is that it is incompatible with some theories of mind, and relies on non-reductive physicalism, which --- while a popular theory --- has come under criticism with several counter arguments in the last few centuries. An argument can be made that any physicalist theory of mind is sufficient for the theory to work: all it relies on is that artificial minds belong in the space of minds with human minds. However, to permit equivalent behaviour across different Sloman subspaces, other physicalist theories such as identity theory require workarounds, because the bridge laws it relies on limit the minds that experience certain phenomena based partly on \emph{hardware}.\par

Rather than detailing potential workarounds here for specific alternative theories of mind, the fact is instead acknowledged; future work can apply similar abstractions over Sloman's space to different theories of mind.\par

% Never a complete fix: just like humans, there's always edge cases
\subsection{The Edge Case}
As may have been noticed already, behaviour formalisms alone can't solve problems in AI safety. One factor which conspires to complicate existential risk research is the difficulty of dealing with edge cases. To demonstrate: should a general artificial intelligence be created, it is unlikely that the algorithms that create it would be contained indefinitely. This is because these technologies to create the intelligence will be rediscovered many times, and commitment to the protection of this data from the public requires potentially international collaboration. As this is typically very difficult to achieve, one cannot rely on it. One must therefore work on the assumption that the public are likely to acquire the technologies necessary for creating their own general intelligences.\par

Any safety measure which is not essential for the creation of a general intelligence should therefore be neglected as a complete solution: someone is likely to leave the algorithms out of their general intelligence, and only one such instance presents a major safety risk.\par

Though it is tempting for this to inspire a rather bleak outlook, many processes may be discovered which may keep humanity safe from an artificial general intelligence. One such process is behaviour formalism. Behaviour formalisms present an opportunity for the constriction of possible behaviour an instance of an intelligence might exhibit, as the specification of the mind within the space of possible minds allows us to formalise anthropomorphic behaviour. Therefore, while these formalisms are not a complete solution to the issue, they are a helpful tool in an intelligence's safe construction.\par

% Currently there aren't too many formalisms yet
\subsection{Limited numbers of formalisms}
One limitation of the theory as it currently stands is that, whole it is valuable in its practical merits, computational formalisms of human behaviour are limited in scope currently. Formalisms of trust are plentiful, and work is done on comfort and responsibility, but the spectrum of human behaviour and experience is hard to formalise. Lots of this work has not yet been undertaken. \par

Over time, this limitation should lessen; regardless of the utility of the philosophical theory, formalisms as a tool for computer scientists, sociologists, and psychologists are useful and can be expected to grow in scope as a result. Therefore, this limitation is expected to be short-lived. Nevertheless, actual solutions to concrete AI problems may not be feasibly produced until a broader range of formalisms have been developed.\par

% How can we quantify behaviour so as to empirically show that the argument regarding qualitative identity holds?
% \subsection{Quantifying Behaviour in Sloman's Space}
% Behaviour is difficult to quantify; in part, this is the value of formalisms, as they provide an algorithmic or mathematical process for quantifying certain behaviours.\par
% 
% Solving the limitation of incompatibility with certain theories of mind (\cref{subsec:incompatibility}), however, gets easier with more strong arguments regarding the qualitative identity of the behaviour emerging from a formalism over different subspaces of minds. One of the reasons these stronger arguments cannot currently be made is that measurement of behaviour in a qualitative way is undeveloped: if this can be given some unit, and measured --- possibly using a process calculus, as algorithmic formalism of behaviour implies that processes can appropriately represent the behaviour of a SIGN NOT SURE WHAT TO SAY

% Not all formalisms will actually be accurate. What happens if we build a ``trust'' formalism that isn't really?
\subsection{Formalism Accuracy}
One issue with formalisms as a field of study is whether they ``accurately'' represent the behaviour they claim to. For example, Eigentrust~\citep{eigentrust} uses the concept of trust a metaphor to draw conclusions from reputation, which it focuses on modelling. The argument often provided in these situations is that the behaviour ``formalised'' is a useful metaphor for an intended outcome or perceived behaviour on the part of the algorithm. Therefore, whether it accurately represents trust is a moot point, as it has no effect on Eigentrust's efficacy in its \emph{intended} purpose.\par

I propose two solutions to this issue:

\begin{enumerate}
  \item The term ``anthropomorphic algorithms'' should apply specifically to behaviour formalisms which have their roots in well-defined sociology, psychology, and anthropology. In this way, researchers and AI developers should be able to distinguish between socially accurate behaviour formalisms and those derived from analogy for engineering purposes.
  \item Development of rigorous formalism analysis in the fields that the formalisms originate from: this would enable a richer and more diverse range of formalisms, which is hard to produce without interdisciplinary collaboration with these social sciences and humanities subjects.
\end{enumerate}


\subsection{An Example: The Electric Monk}
In the novel ``Dirk Gently's Holistic Detective Agency'' by Douglas Adams, there is presented a character who is useful as an example of anthropomorphic algorithms at work. ``The Electric Monk'' is a commercial artificial intelligence which is designed for the purpose of alleviating the burden of ``believing'' in things from the owner, similarly to how a television recorder might ``watch television'' for the owner.\par

This Electric Monk is responsible for killing a local businessman, due to its being told to ``shoot off'' (as in go away) and taking the instruction literally. It then believed that it was imperative that it shoot something.\par

The Electric Monk is an excellent example of anthropomorphic algorithms' strengths and weaknesses. Its limited anthropomorphism presents some utility, but its intelligence also makes it dangerous. As a result of its limited anthropomorphism, it lacks the capacity to act in guaranteed safe ways. One is inclined to explore how anthropomorphic algorithms might be applied safely in this case.\par

\subsubsection{Incitement to act}
The Electric Monk would be harmless if it was incapable of \emph{action}. In reality, the monk's prescribed activity --- that of belief --- is an action with no output, similar to a speech act\footnote{A speech act is the sort of action discharged implicitly when certain things are spoken, but which have no tangible effect. An example would be ``I now pronounce you man and wife'' --- this is rather meaningless if spoken by most people, but by a priest on an altar in front of a man and a woman, the words cause something to happen which is non-tangible. The Electric Monk's belief is similar: it has no impact physically, and no output, but it has value in the fictional world.}. An agent which need produce no output, then, might be limited such that it cannot act.\par

Of course, this limitation is meaningless for almost all realistic agents; certainly, intelligent agents can be made safe by neutering their ability to do anything at all, but one supposes this would strip the utility from them somewhat. How can we make agents which are incited to act safe?\par

One way to do this would be to observe what traits humans rely on for the safety of their actions, and to implement formalisms of those traits. So, if the Electric Monk were allowed to speak, traits humans use for safety in this case would be implemented. These traits might include responsibility for their actions, and an ethical understanding taking into account expected consequence and the net positive or negative effect of those actions. By characterising the traits humans use when acting, and implementing those traits, one can imagine that the artificial agent would be at least as safe as an equivalent human agent\footnote{And potentially safer, as it would likely be be possible to implement the formalisms with more accuracy than a human agent might --- humans can ignore these traits, but no such ability might be made available to the intelligent agent}.\par

\subsubsection{More formalisms necessary}
The Electric Monk's belief formalism is all that at acts on, but this is also the only formalism it is given; it shows little other human-like behaviour (save walking and talking). It is apparent that a single formalism might often be abused so as to permit unsafe behaviour. An emotional agent might understand happiness and sadness in human-like ways, so as to maximise happiness: this is dangerous in the same way as the paperclip maximiser.\par

However, Electric Monk-like agents will surely come to be in the future, particularly with the development of general intelligence. Therefore, the combination of multiple formalisms in any anthropomorphic agent seems an appropriate safety precaution. This way, an equilibrium might be reached though nuances of the behaviours' interactions together. For example, a happiness-maximising agent is effectively a utilitarian agent\todo{Is this correct terminology?}, but real-world utilitarians might be limited by laziness --- preventing those human agents from performing tasks which are lots of effort. Effort formalisms might limit the safety issues concerning a utilitarian agent in this case. \par

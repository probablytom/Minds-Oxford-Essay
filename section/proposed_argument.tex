\section{Abstracting over Sloman's space}

Sloman's space can be envisaged as a literal mathematical space, where every point describes a certain mind. We can see that there is no guarantee that a subspace of human minds and a subspace of artificial minds --- minds born from simple intelligent agents, for example --- will have anything in common mathematically. Certainly, one cannot assert that numerically identical minds exist in both subspaces, as two numerically identical minds would, by definition, occupy the same point in Sloman's space.\par

However, Sloman's space describes the fundamental properties of a mind; emergent phenomena\footnote{Emergent phenomena are generally complex phenomena where the same behaviour can arise from different initial conditions, or entirely different systems. They are often studied in sociotechnical systems research, which analyses systems of interrelated social and technical components working in synchrony.} are unsuitable as dimensions of the space, as they can be affected by many independent variables. Note also that these emergent phenomena can arise in the same way in different minds. Kripke was the first to discuss that a behavioural artifact with the same rigid designator can arise from different minds~\citep{kripke1972naming} --- a simple corollary being that behaviour cannot distinguish different minds, as their properties can converge on the same behaviour.\par

\subsection{Anthropomorphic Algorithms for Specifying Behaviour}

According to Castelfranchi \& Falcone~\citep{CastelfranchiSocialApproach}, trust arises from an assessment of the capability and will of a trusted agent to achieve goals of the trustee. Luhmann, a sociologist, asserts that trust is a form of social risk aversion~\citep{luhmann2000familiarity}, while Deutsch, a psychologist, indicates that trust incorporates utility as well as risk in different ways, expressed as confidence, gambling, masochism and other behavioural tendencies~\citep{deutsch1962cooperation}. Some sociotechnical work attempts to bridge the divide between the sociological and psychological approaches through algorithmic formalism~\citep{Marsh1994FormalisingConcept}.\par

Trust is a well-studied topic with a range of literature to be drawn from. More importantly to the argument at hand, it is an emergent phenomena which manifests in different ways and is born from no clear underlying properties of a mind. However, formalisms of trust --- and how trusting agents behave --- can be found in Castelfranchi and Falcone's work, as well as Marsh's, and plenty of others. One can assert that these formalisms describe an aspect of behaviour in humans (with the exception of some types of minds such as human sociopaths, whose behaviour one might expect to be abnormal). Many formalisms listed are an algorithmic formalisms, meaning that they can be implemented so as to direct the behaviour of an intelligent agent in a more anthropomorphic way. They are therefore the canonical example of the anthropomorphic algorithm, and happen to be the most extensively studied.\par

A given anthropomorphic algorithm wouldn't apply to simply any type of mind: caveats such as sociopaths are to be expected, and so a domain --- a subspace of Sloman's space --- is suitable as a way to limit the scope of the formalism. Trust formalisms could then be seen as algorithms (or ordinary mathematical functions) over a subspace of minds which describe behaviour. However, from existing anthropomorphic algorithms such as trust, we can see that this domain hints toward making Sloman's space more useful philosophically: the domain of a trust formalism such as Marsh's is to apply to not only neuronormative humans, but intelligent agents, also.\par

The mind of an intelligent agent and a human may be markedly different, but they can give rise to the same emergent phenomena. Importantly, the formalisms might manifest in numerically non-identical ways: the way an intelligent agent computes trust is rather different to how the human brain does, even under the same formalism, due to differences in hardware and in implementation detail. However, they conform to the same description of the phenomenon they exhibit --- the formalism --- and should behave in the same way as a result. One might say that the minds trust in \emph{qualitatively identical} ways, because they follow a numerically identical formalism, with differences arising in implementation of the overarching theory.\par

Using anthropomorphic algorithms, then, the philosopher can assert behavioural identity between different types of minds. This abstraction over Sloman's space means that the space's indeterminate structure no longer prevents asserting these identities, precisely because the space --- regardless of structure --- allows for the definition of the domains of formalisms which describe an agent's behaviour.\par

\subsection{Compatibility with Theories of Mind}
To a degree, this thesis depends on non-reductive physicalism: for a behaviour to apply to a domain of minds, there has to be nothing non-physical that affects that behaviour, as this would imply a behaviour cannot be shared by humans and computers. However, that the thesis is compatible with non-reductive physicalism means that it can co-exist with existing literature on theory of mind, and that the theories of mind it relies on are at least partially defined already.\par

Non-reductive physicalism works well as a foundation for the notion that non-biological systems --- such as computers --- might also have minds. For that reason, that fact that the application of anthropomorphic algorithms for behaviour specification works well with non-reductive physicalism helps to present a more whole notion of how a behaviour might arise from a computational system.\par

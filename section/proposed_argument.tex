\section{Abstracting over Sloman's space}

Sloman's space can be envisaged as a literal mathematical space, where every point describes a certain mind. We can see that there is no guarantee that a subspace of human minds and a subspace of artificial minds --- minds born from simple intelligent agents, for example --- will have anything in common mathematically. Certainly, one cannot assert that numerically identical minds exist in both subspaces\todo{Discuss what numerical and qualitative identity are}, as this would require them to overlap: two numerically identical minds would, by definition, occupy the same point in Sloman's space. As we can assert nothing about the structure of Sloman's space, this overlap has no guarantee.\par

However, Sloman's space describes the fundamental properties of a mind; emergent phenomena are unsuitable as dimensions of the space, as they can be affected by many independent variables. Note also that these emergent phenomena can arise in the same way in different minds. One might say, ``Alice and Bob are just as greedy as each other'', without implying anything about fundamental traits of Alice and Bob. ``Greed'' is a property they share, but their greed can arise for a plethora of reasons --- because greed is a phenomenon which emerges from fundamental traits of their minds.\par

This is true of other traits, too. For example, according to Castelfranchi \& Falcone~\citep{CastelfranchiSocialApproach}, trust arises from an assessment of the capability and will of a trusted agent to achieve goals of the trustee. Luhmann, a sociologist, asserts that trust is a form of social risk aversion~\citep{luhmann2000familiarity}, while Deutsch, a psychologist, indicates that trust incorporates utility as well as risk in different ways, expressed as confidence, gambling, masochism and other behavioural tendencies~\citep{deutsch1962cooperation}.\par

Trust is a well-studied topic with a range of literature to be drawn from. More importantly to the argument at hand, it is an emergent phenomena which manifests in different ways and is born from no clear underlying properties of a mind. However, formalisms of trust --- and how trusting agents behave --- can be found in Castelfranchi and Falcone's work, as well as Marsh's, and plenty of others. One can assert that these formalisms describe an aspect of behaviour in humans, with the exception of some types of minds such as human sociopaths, whose behaviour one might expect to be abnormal: this is precisely their goal. Every formalism listed, however, is also an algorithmic formalism, meaning that it can be implemented so as to direct the behaviour of an intelligent agent in a more anthropomorphic way. They are therefore the canonical example of the anthropomorphic algorithm, and happen to be the most extensively studied.\par

A given anthropomorphic algorithm wouldn't apply to any type of mind: caveats such as sociopaths are to be expected, and so a domain --- a subspace of Sloman's space --- is suitable as a way to limit the scope of the formalism. Trust formalisms could then be seen as algorithms (or ordinary mathematical functions) over a subspace of minds which describe behaviour. However, from existing anthropomorphic algorithms such as trust, we can see that this domain hints toward making Sloman's space more useful philosophically: the domain of a trust formalism such as Marsh's is to apply to not only neuronormative humans, but intelligent agents, also.\par

The mind of an intelligent agent and a human may be markedly different, but they \emph{can} give rise to the same emergent phenomena. Importantly, the formalisms might manifest in numerically non-identical ways: the way an intelligent agent computes trust is rather different to how the human brain does, even under the same formalism, due to differences in hardware and in implementation detail. However, they conform to the same description of the phenomenon they exhibit --- the formalism --- and should behave in the same way as a result. One might say that the minds trust in qualitatively identical ways, and follow a numerically identical formalism, with differences arising in implementation of the overarching theory.\par

Using anthropomorphic algorithms, then, the philosopher can assert behavioural identity between different types of minds. This abstraction over Sloman's space means that the space's indeterminate structure no longer prevents asserting these identities, precisely because the space --- regardless of structure --- allows for the definition of the domains of formalisms which describe an agent's behaviour.\par

\subsection{Compatibility with Theories of Mind}
To a degree, this thesis depends on non-reductive physicalism: for a behaviour to apply to a domain of minds, there has to be nothing non-physical that affects that behaviour, since a non-physical functional state could affect the behaviour (by its functional definition). This means, however, that the thesis is compatible with non-reductive physicalism means that it can co-exist with existing literature on theory of mind.\par

Non-reductive physicalism works well as a foundation for the notion that non-biological systems --- such as computers --- might also have minds. For that reason, that fact that the application of anthropomorphic algorithms for behaviour specification works well with non-reductive physicalism helps to present a more whole notion of how a behaviour might arise from a computational system.\par

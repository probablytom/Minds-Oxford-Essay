\section{Problem Outline}
\subsection{Existential Risk and AI Safety}
Research on existential risk has increasingly turned an eye toward problems of safety regarding artificial intelligence. This research suffers some difficult challenges. For one, practical exploration of what is often termed ``strong AI'' --- an artificially intelligent agent which has a ``mind'' and mental states --- cannot be explored by concrete example. Rather, researchers must obliquely attack the problem by observing how minds in humans (and other conscious animals) appear to operate.\par

\cite{Sloman1984TheMinds} presents an approach to the problem whereby a space of possible minds is envisaged. This approach is useful when describing some of AI safety's most interesting problems, such as the paperclip maximiser~\citep{bostrom2003ethical}. Paperclip maximisers are intelligences which pursue goals --- such as the creation of ever more paperclips --- while showing indifference to any consequences, such as the eventual mulching of humans into stationary.\par

Whether the paperclip maximiser problem is likely to occur is irrelevant; it shows that there are issues with the behaviour we might expect a very intelligent agent to express. More recently, concrete issues in artificial intelligence safety have been identified which researchers can work on today.\par

An example is found in reward hacking~\citep{concrete_problems}, where an agent's behaviour might move toward unintended end-states so as to satisfy the criteria for its fulfilment of a goal, without achieving the goal intended to be set for the agent. Another example might be that of agent corrigibility~\citep{corrigibility}, where an agent acting in unexpected ways might not accept corrective intervention on the part of its creators. Finding solutions to these problems is a challenging undertaking, yet is also imperative for a future with safe AI.\par

\subsection{A Philosophical Approach to Attacking the Problems}
Existential Risk issues are often approached from a philosophical angle, shedding light on important issues and directing thought on the issue toward likely solutions. In AI safety, philosophical approaches are of paramount importance, as sufficiently advanced intelligent systems have not been developed to such a degree that their intellect might be comparable to an ordinary human mind. Therefore, empirical study can be impractical, and a-priori analysis through philosophy is a researcher's most valuable tool.\par

Tools like Sloman's Space of Possible Minds\footnote{For brevity and readability, Sloman's Space of Possible Minds will be referred to as simply ``Sloman's space'' through this essay.} can be of some help in reasoning about artificial intelligence. Though Sloman doesn't state much about practical structures of his space, thinking in terms of the ``dimensions'' a mind might have --- its social capability, or its sense of self preservation, for example --- help to reason about the traits differentiating types of minds\footnote{It's worth noting that some theorise that sufficiently intelligent minds converge on similar properties, such as self preservation. ``Instrumental Convergence'' as it pertains to artificial intelligence is explored in~\cite{basic_ai_drives}.}. Similarities and differences between minds can be considered be geometric differences within the space; therefore, minds with similar qualities would appear closer together than others in the theoretical space, and the closer two minds were, the closer their qualities would be\footnote{The space's natural geometric properties are a useful feature of Sloman's approach. For example, one might be inclined to take the euclidean distance between two minds in the space as a naive measure of how different they are.}.\par

Unfortunately, reliably determining the structure and nature of Sloman's space, with little empirical work possible, and with the very nature of a ``mind'' a philosophical quandary, is an impossible feat today. Therefore, as a technique, Sloman's space has issues which limit how practically a philosopher might reason using it.\par

\subsection{Introducing Anthropomorphic Algorithms}
Anthropomorphic Algorithms are algorithms designed to guide an intelligent agent's behaviour in more human-like ways. Existing Anthropomorphic Algorithms include Marsh's formalism of trust~\citep{Marsh1994FormalisingConcept} or Eigentrust~\citep{eigentrust}, which both simulate ``trusting'' behaviour. Indeed, trust formalisms are possibly the most widely researched anthropomorphic algorithms. Some formalisms of trust --- such as Marsh's --- attempt to describe the sociological and psychological factors of trust in humans, and later use this formalism to quantify trust in some way, building algorithms around the quantification such that intelligent agents might exhibit the same behaviour\footnote{As anthropomorphic algorithms are generally implementations of a formalism of some human behavioural trait, ``anthropomorphic algorithm'' and ``formalism'' will often be used interchangeably through this essay.}. This means that, while their applications in computing science are broad, they have applications in other fields also, such as the social sciences. Simpler formalisms also exist, such as a formalism of bias in reasoning~\citep{armstrong_bias}, which consists of simple mathematics and requires only a few paragraphs of reasoning to fully explain.\par

The anthropomorphic algorithm's very existence, and the formalism of a certain behaviour across different types of minds, presents an opportunity for philosophers to make use of Sloman's space as a powerful tool in reasoning about AI safety, and about theories of mind generally. Section 2 will explore how using computational formalisms of human traits can act as a general specifier of behaviour, and allow us to reason about emergent phenomena from Sloman's space. Section 3 will explore how this thesis can be applied practically, and section 4 will explore limitations regarding the application of the theory as it presently stands, as well as future work solving these limitations. The essay concludes in section 5 with a discussion of the theory and its implications.\par

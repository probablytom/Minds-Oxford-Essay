\section{Problem Outline}
\subsection{Existential Risk and AI Safety}
Research on existential risk has increasingly turned an eye toward problems of safety regarding artificial intelligence. This research suffers some difficult challenges. For one, practical exploration of what is often termed ``strong AI'' --- an artificially intelligent agent which has a ``mind'' a mental states --- cannot be explored by concrete example. Rather, researchers must obliquely attack the problem by observing how minds in humans (and other conscious animals) appear to operate.\par

Some examples of ways to approach this problem present useful tools to the curious philosopher. \cite{Sloman1984TheMinds} presents an approach whereby a space of possible minds is envisaged. This approach is useful when describing some of AI safety's most interesting problems, such as the paperclip maximiser~\citep{bostrom2003ethical}. The principle of the argument is that, when giving a superintelligent agent the task of making as many paperclips as it can, it will consume any and all resources present for the construction of paperclips --- including, for example, iron in human blood.\par

Whether the paperclip maximiser problem is likely to occur is irrelevant; it shows that there are issues with the behaviour we might expect a very intelligent agent to express. More recently, concrete issues in artificial intelligence safety have been identified which researchers can work on today. \par

An example is found in reward hacking~\citep{concrete_problems}, where an agent's behaviour might tend toward unintended end-states so as to satisfy the criteria for its fulfilment of a goal, without achieving the goal intended to be set for the agent. An example might be an agent told to tidy mess left by a child, which identifies a messy room by whether it can see objects lying on the floor. This agent may ``achieve'' its goals by detaching its robotic eyes. As it cannot see any objects lying on the floor, it must conclude that its goal has been achieved --- but mess can still be found on the floor. This particular behaviour seems harmless, but unintended behaviour and end-states might end badly for human agents in other situations. Another example might be that of agent corrigibility~\citep{corrigibility}, where an agent acting in unexpected ways might not accept corrective intervention on the part of its creators. This agent might note that a change to its goals does not help to achieve its currently assigned goal; therefore, to achieve its goal, it is inclined to ignore attempts to alter its behaviour by its creators.\par

\subsection{A Philosophical Approach to Attacking the Problems}
Existential Risk issues are often approached from a philosophical angle, shedding light on important issues and directing thought on the issue toward likely solutions. In AI safety, philosophical approaches are of paramount importance, as sufficiently advances intelligent systems have not been developed to such a degree that their intellect might be comparable to an ordinary human mind. The empirical assessment of general intelligence might come about as techniques such as whole brain emulation become viable for humans, or as general artificial intelligence comes to the fore as computer-scientific techniques advance.\par

Tools like Sloman's Space of Possible Minds\footnote{For brevity and readability, Sloman's Space of Possible Minds will be referred to as simply ``Sloman's space'' through this essay.} can be of some help in reasoning about artificial intelligence. Though Sloman doesn't state much about practical structures of his space, thinking in terms of the ``dimensions'' a mind might have --- its social capability, or its sense of self preservation, for example --- help to reason about the differences different mind types might have. A machine might not have much reason to preserve itself, particularly when considering that what makes it conscious can presumably be backed up, or that multiple copies of it attempting to accomplish identical goals exist\footnote{It's worth noting that some theorise that sufficiently intelligent minds converge on similar properties, such as self preservation. ``Instrumental Convergence'' as it pertains to artificial intelligence is well explored in \cite{basic_ai_drives}.}. Similarities and differences between minds would be geometric differences within the space; therefore, minds with similar qualities would appear closer together than others in the theoretical space, and the closer two minds were, the closer their qualities would be\footnote{The space's natural geometric properties are a useful feature of Sloman's approach. For example, one might be inclined to take the euclidean distance between two minds in the space as a naive measure of how different they are.}.\par

Unfortunately, reliably determining the structure and nature of Sloman's space, with little empirical work possible, and with the very nature of a ``mind'' a philosophical quandary, is an impossible feat today. Therefore, as a technique, Sloman's space has issues which limit how practically a philosopher might reason using it.\par

\subsection{Introducing Anthropomorphic Algorithms}
Anthropomorphic Algorithms are algorithms designed to guide an intelligent agent's behaviour in more human-like ways. Existing Anthropomorphic Algorithms include Marsh's formalism of trust\citep{Marsh1994FormalisingConcept} or Eigentrust\citep{eigentrust}, which both simulate ``trusting'' behaviour. Indeed, trust formalisms are possibly the most widely researched anthropomorphic algorithms. Some formalisms of trust --- such as Marsh's --- attempt to formalise the sociological and psychological factors of trust in humans, and later use this formalism to quantify trust in some way, and build algorithms around the quantification such that intelligent agents might exhibit the same behaviour. This means that, while their applications in computing science are broad, they have applications in other fields also --- such as the social sciences.\par

The anthropomorphic algorithm's very existence, and the formalism of a certain behaviour across different types of minds, presents an opportunity for philosophers to make use of Sloman's space as a powerful tool in reasoning about AI safety, and about theories of mind generally. \todo{SPECIFY WHAT I'LL DETAIL IN EACH SECTION AND WHAT WILL BE PRESENTED THROUGH THE ESSAY}
